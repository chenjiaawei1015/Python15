# 进程

## multiprocess模块

- 仔细说来，multiprocess不是一个模块而是python中一个操作、管理进程的包。 之所以叫multi是取自multiple的多功能的意思,在这个包中几乎包含了和进程有关的所有子模块。由于提供的子模块非常多，为了方便大家归类记忆，我将这部分大致分为四个部分：创建进程部分，进程同步部分，进程池部分，进程之间数据共享

### multiprocess.process模块

process模块是一个创建进程的模块，借助这个模块，就可以完成进程的创建

- 初始化参数

  ```
  Process([group [, target [, name [, args [, kwargs]]]]])，由该类实例化得到的对象，表示一个子进程中的任务（尚未启动）
  
  强调：
  1. 需要使用关键字的方式来指定参数
  2. args指定的为传给target函数的位置参数，是一个元组形式，必须有逗号
  
  参数介绍：
  1 group参数未使用，值始终为None
  2 target表示调用对象，即子进程要执行的任务
  3 args表示调用对象的位置参数元组，args=(1,2,'egon',)
  4 kwargs表示调用对象的字典,kwargs={'name':'egon','age':18}
  5 name为子进程的名称
  ```

- 方法介绍

  ```
  p.start()：启动进程，并调用该子进程中的p.run() 
  
  p.run():进程启动时运行的方法，正是它去调用target指定的函数，我们自定义类的类中一定要实现该方法  
  
  p.terminate():强制终止进程p，不会进行任何清理操作，如果p创建了子进程，该子进程就成了僵尸进程，使用该方法需要特别小心这种情况。如果p还保存了一个锁那么也将不会被释放，进而导致死锁
  
  p.is_alive():如果p仍然运行，返回True
  
  p.join([timeout]):主线程等待p终止（强调：是主线程处于等的状态，而p是处于运行的状态）。timeout是可选的超时时间，需要强调的是，p.join只能join住start开启的进程，而不能join住run开启的进程 
  ```

- 属性介绍

  ```
  p.daemon：默认值为False，如果设为True，代表p为后台运行的守护进程，当p的父进程终止时，p也随之终止，并且设定为True后，p不能创建自己的新进程，必须在p.start()之前设置
  
  p.name:进程的名称
  
  p.pid：进程的pid
  
  p.exitcode:进程在运行时为None、如果为–N，表示被信号N结束(了解即可)
  
  p.authkey:进程的身份验证键,默认是由os.urandom()随机生成的32字符的字符串。这个键的用途是为涉及网络连接的底层进程间通信提供安全性，这类连接只有在具有相同的身份验证键时才能成功（了解即可）
  ```

- windows使用 process 的注意事项

  ```
  在Windows操作系统中由于没有fork(linux操作系统中创建进程的机制)，在创建子进程的时候会自动 import 启动它的这个文件，而在 import 的时候又执行了整个文件。
  
  因此如果将process()直接写在文件中就会无限递归创建子进程报错。所以必须把创建子进程的部分使用if __name__ ==‘__main__’ 判断保护起来，import 的时候  ，就不会递归运行了
  ```

## 进程操作

- 在python中启动的第一个子进程

  ```python
  import time
  from multiprocessing import Process
  
  def f(name):
      print('hello', name)
      print('我是子进程')
  
  if __name__ == '__main__':
      p = Process(target=f, args=('bob',))
      p.start()
      time.sleep(1)
      print('执行主进程的内容了')
  ```

- join方法

  ```python
  import time
  from multiprocessing import Process
  
  def f(name):
      print('hello', name)
      time.sleep(1)
      print('我是子进程')
  
  
  if __name__ == '__main__':
      p = Process(target=f, args=('bob',))
      p.start()
      #p.join()
      print('我是父进程')
  ```

- 查看主进程和子进程的进程号

  ```python
  import os
  from multiprocessing import Process
  
  def f(x):
      print('子进程id ：',os.getpid(),'父进程id ：',os.getppid())
      return x*x
  
  if __name__ == '__main__':
      print('主进程id ：', os.getpid())
      p_lst = []
      for i in range(5):
          p = Process(target=f, args=(i,))
          p.start()
  ```

- 多个进程同时运行

  ```python
  import time
  from multiprocessing import Process
  
  
  def f(name):
      print('hello', name)
      time.sleep(1)
  
  
  if __name__ == '__main__':
      p_lst = []
      for i in range(5):
          p = Process(target=f, args=('bob',))
          p.start()
          p_lst.append(p)
  ```

- 多个进程同时运行，再谈join方法

  ```python
  import time
  from multiprocessing import Process
  
  
  def f(name):
      print('hello', name)
      time.sleep(1)
  
  
  if __name__ == '__main__':
      p_lst = []
      for i in range(5):
          p = Process(target=f, args=('bob',))
          p.start()
          p_lst.append(p)
          p.join()
      # [p.join() for p in p_lst]
      print('父进程在执行')
  ```

  ```python
  import time
  from multiprocessing import Process
  
  def f(name):
      print('hello', name)
      time.sleep(1)
  
  if __name__ == '__main__':
      p_lst = []
      for i in range(5):
          p = Process(target=f, args=('bob',))
          p.start()
          p_lst.append(p)
      # [p.join() for p in p_lst]
      print('父进程在执行')
  ```

- 通过继承Process类开启进程

  ```python
  import os
  from multiprocessing import Process
  
  
  class MyProcess(Process):
      def __init__(self,name):
          super().__init__()
          self.name=name
          
      def run(self):
          print(os.getpid())
          print('%s 正在和女主播聊天' %self.name)
  
  p1=MyProcess('wupeiqi')
  p2=MyProcess('yuanhao')
  p3=MyProcess('nezha')
  
  p1.start() #start会自动调用run
  p2.start()
  # p2.run()
  p3.start()
  
  
  p1.join()
  p2.join()
  p3.join()
  
  print('主线程')
  ```

- 进程之间的数据隔离问题

  ```python
  from multiprocessing import Process
  
  def work():
      global n
      n=0
      print('子进程内: ',n)
  
  
  if __name__ == '__main__':
      n = 100
      p=Process(target=work)
      p.start()
      print('主进程内: ',n)
  ```

## 守护进程

- 会随着主进程的结束而结束

- 主进程创建守护进程

  1. 守护进程会在主进程代码执行结束后就终止
  2. 守护进程内无法再开启子进程,否则抛出异常：AssertionError: daemonic processes are not allowed to have children
  3. **进程之间是互相独立的，主进程代码运行结束，守护进程随即终止**

- 没有设置守护进程的情况下, 主进程执行完毕后, 子线程仍在运行, 主进程会等待所有的子进程任务执行完毕才会终止

  ```python
  from multiprocessing import Process
  import time
  
  
  def func(name):
      time.sleep(2)
      print(name)
  
  
  if __name__ == '__main__':
      process_list = []
      for index in range(5):
          p = Process(target=func, args=(f'进程{index}',))
          process_list.append(p)
  
      for item in process_list:
          item.start()
  
      print('主进程执行完毕')
  
  # 主进程执行完毕
  # 进程0
  # 进程1
  # 进程3
  # 进程2
  # 进程4
  ```

- 如果为守护进程, 主进程执行完毕, 守护进程也就完毕了

  ```python
  from multiprocessing import Process
  import time
  
  
  def func(name):
      time.sleep(2)
      print(name)
  
  
  if __name__ == '__main__':
      process_list = []
      for index in range(5):
          p = Process(target=func, args=(f'进程{index}',))
          # 设置为守护进程
          p.daemon = True
          process_list.append(p)
  
      for item in process_list:
          item.start()
  
      print('主进程执行完毕')
  
  # 主进程执行完毕
  ```

## 进程的其他方法

### terminate 关闭进程

```python
from multiprocessing import Process
import time


def func(name):
    time.sleep(3)
    print(name)


if __name__ == '__main__':
    process_list = []
    for index in range(5):
        p = Process(target=func, args=(f'进程{index}',))
        process_list.append(p)

    for item in process_list:
        item.start()

    for index, item in enumerate(process_list):
        if index % 2:
            # 关闭进程
            item.terminate()

    print('主进程执行完毕')

# 注意:进程的操作是通过调用操作系统来完成的, 并不是马上能关闭

# 主进程执行完毕
# 进程0
# 进程2
# 进程4
```

### is_alive 进程是否存活

```python
from multiprocessing import Process
import time


def func(name):
    time.sleep(10)
    print(name)


if __name__ == '__main__':
    process_list = []
    for index in range(5):
        p = Process(target=func, args=(f'进程{index}',), name=f'进程{index}')
        process_list.append(p)

    for item in process_list:
        item.start()

    for index, item in enumerate(process_list):
        if index % 2:
            # 关闭进程
            item.terminate()

    time.sleep(3)

    for item in process_list:
        if item.is_alive():
            print(f'{item.name} 存活')
        else:
            print(f'{item.name} 销毁')

    print('主进程执行完毕')

# 进程0 存活
# 进程1 销毁
# 进程2 存活
# 进程3 销毁
# 进程4 存活
# 主进程执行完毕
# 进程0
# 进程2
# 进程4
```

## 锁

- 程序的异步，让多个任务可以同时在几个进程中并发处理，他们之间的运行没有顺序，一旦开启也不受我们控制。尽管并发编程让我们能更加充分的利用IO资源，但是也给我们带来了新的问题
- 当多个进程使用同一份数据资源的时候，就会引发数据安全或顺序混乱问题

### 多进程抢占输出资源

```python
import os
import time
import random
from multiprocessing import Process


def work(n):
    print('%s: %s is running' % (n, os.getpid()))
    time.sleep(random.random())
    print('%s:%s is done' % (n, os.getpid()))


if __name__ == '__main__':
    for i in range(3):
        p = Process(target=work, args=(i,))
        p.start()
```

### 使用锁维护执行顺序

```python
# 由并发变成了串行,牺牲了运行效率,但避免了竞争
import os
import time
import random
from multiprocessing import Process, Lock


def work(lock, n):
    lock.acquire()
    print('%s: %s is running' % (n, os.getpid()))
    time.sleep(random.random())
    print('%s: %s is done' % (n, os.getpid()))
    lock.release()


if __name__ == '__main__':
    lock = Lock()
    for i in range(3):
        p = Process(target=work, args=(lock, i))
        p.start()

# 0: 5242 is running
# 0: 5242 is done
# 1: 5243 is running
# 1: 5243 is done
# 2: 5244 is running
# 2: 5244 is done
```

上面这种情况虽然使用加锁的形式实现了顺序的执行，但是程序又重新变成串行了，这样确实会浪费了时间，却保证了数据的安全

### 多进程实现抢票

- 没有加锁的情况(数据不安全)

  ```python
  from multiprocessing import Process
  from multiprocessing import Lock
  from config import common
  import time
  import random
  import json
  
  
  def get_ticket():
      with open(common.TICKET_FILE_PATH, 'r', encoding=common.ENCODING) as src_f:
          json_data = json.load(src_f)
      return json_data['ticket_num']
  
  
  def write_ticket(num):
      with open(common.TICKET_FILE_PATH, 'w', encoding=common.ENCODING) as dst_f:
          ticket_dict = {'ticket_num': num}
          json.dump(ticket_dict, dst_f)
  
  
  def take_ticket(name):
      ticket_num = get_ticket()
      if ticket_num <= 0:
          print(f'{name} 进行抢票, 但是没票了')
          return
  
      time.sleep(random.randint(0, 2))
  
      ticket_num -= 1
      print(f'{name} 抢到了票, 还剩余 {ticket_num}')
      write_ticket(ticket_num)
  
  
  if __name__ == '__main__':
      process_list = []
      for index in range(10):
          p = Process(target=take_ticket, args=(f'{index}',))
          process_list.append(p)
  
      for item in process_list:
          time.sleep(random.random())
          item.start()
  
  # 0 抢到了票, 还剩余 4
  # 1 抢到了票, 还剩余 3
  # 3 抢到了票, 还剩余 2
  # 4 抢到了票, 还剩余 1
  # 5 抢到了票, 还剩余 0
  # 6 进行抢票, 但是没票了
  # 2 抢到了票, 还剩余 2
  # 8 抢到了票, 还剩余 1
  # 7 抢到了票, 还剩余 1
  # 9 抢到了票, 还剩余 0
  ```

- 加锁的情形(保证了数据的安全)

  ```python
  from multiprocessing import Process
  from multiprocessing import Lock
  from config import common
  import time
  import random
  import json
  
  
  def get_ticket():
      with open(common.TICKET_FILE_PATH, 'r', encoding=common.ENCODING) as src_f:
          json_data = json.load(src_f)
      return json_data['ticket_num']
  
  
  def write_ticket(num):
      with open(common.TICKET_FILE_PATH, 'w', encoding=common.ENCODING) as dst_f:
          ticket_dict = {'ticket_num': num}
          json.dump(ticket_dict, dst_f)
  
  
  def take_ticket(name, ticket_lock):
      # 申请锁
      ticket_lock.acquire()
      ticket_num = get_ticket()
      if ticket_num <= 0:
          print(f'{name} 进行抢票, 但是没票了')
      else:
          time.sleep(random.randint(0, 2))
  
          ticket_num -= 1
          print(f'{name} 抢到了票, 还剩余 {ticket_num}')
          write_ticket(ticket_num)
      # 释放锁
      ticket_lock.release()
  
  
  if __name__ == '__main__':
      ticket_lock = Lock()
      process_list = []
      for index in range(10):
          p = Process(target=take_ticket, args=(f'{index}', ticket_lock))
          process_list.append(p)
  
      for item in process_list:
          time.sleep(random.random())
          item.start()
  
  # 0 抢到了票, 还剩余 4
  # 1 抢到了票, 还剩余 3
  # 2 抢到了票, 还剩余 2
  # 3 抢到了票, 还剩余 1
  # 4 抢到了票, 还剩余 0
  # 5 进行抢票, 但是没票了
  # 6 进行抢票, 但是没票了
  # 7 进行抢票, 但是没票了
  # 8 进行抢票, 但是没票了
  # 9 进行抢票, 但是没票了
  ```

- 加锁可以保证多个进程修改同一块数据时，同一时间只能有一个任务可以进行修改，即串行的修改，没错，速度是慢了，但牺牲了速度却保证了数据安全.虽然可以用文件共享数据实现进程间通信，但问题是：
  1. 效率低（共享数据基于文件，而文件是硬盘上的数据）
  2. 需要自己加锁处理
- 因此我们最好找寻一种解决方案能够兼顾：1、效率高（多个进程共享一块内存的数据）2、帮我们处理好锁问题。这就是mutiprocessing模块为我们提供的基于消息的IPC通信机制：队列和管道
  - 队列和管道都是将数据存放于内存中
  - 队列又是基于(管道+锁)实现的,可以让我们从复杂的锁问题中解脱出来
  - 我们应该尽量避免使用共享数据,尽可能使用消息传递和队列,避免处理复杂的同步和锁问题,而且在进程数目增多时,往往可以获得更好的可获展性

## 信号量

- 互斥锁同时只允许一个线程更改数据，而信号量Semaphore是同时允许一定数量的线程更改数据
- 信号量同步基于内部计数器，每调用一次acquire()，计数器减1；每调用一次release()，计数器加1.当计数器为0时，acquire()调用被阻塞。这是迪科斯彻（Dijkstra）信号量概念P()和V()的Python实现。信号量同步机制适用于访问像服务器这样的有限资源。
- 信号量与进程池的概念很像，但是要区分开，信号量涉及到加锁的概念

```python
from multiprocessing import Process, Semaphore
import time, random


def go_ktv(sem, user):
    sem.acquire()

    print('%s 占到一间ktv小屋' % user)
    # 模拟每个人在ktv中待的时间不同
    time.sleep(random.randint(1, 3))

    sem.release()


if __name__ == '__main__':
    # 最多允许4个对象拿到锁
    sem = Semaphore(4)

    process_list = []
    for i in range(13):
        p = Process(target=go_ktv, args=(sem, 'user%s' % i,))
        process_list.append(p)

    for item in process_list:
        item.start()
```

## 事件

- python线程的事件用于主线程控制其他线程的执行，事件主要提供了三个方法 set、wait、clear

- 事件处理的机制：全局定义了一个“Flag”，如果“Flag”值为 False，那么当程序执行 event.wait 方法时就会阻塞，如果“Flag”值为True，那么event.wait 方法时便不再阻塞

- ```
  clear：将“Flag”设置为False
  set：将“Flag”设置为True
  ```

### 红绿灯案例

```python
from multiprocessing import Process, Event
import time, random


def car(e, n):
    while True:
        # 进程刚开启，is_set()的值是Flase，模拟信号灯为红色
        if not e.is_set():
            print('\033[31m红灯亮\033[0m，car%s等着' % n)

            # 阻塞，等待is_set()的值变成True，模拟信号灯为绿色
            e.wait()

            print('\033[32m车%s 看见绿灯亮了\033[0m' % n)
            time.sleep(random.randint(3, 6))

            # 如果is_set()的值是Flase，也就是红灯，仍然回到while语句开始
            if not e.is_set():
                continue

            print('车开远了,car', n)
            break


def police_car(e, n):
    while True:
        # 进程刚开启，is_set()的值是Flase，模拟信号灯为红色
        if not e.is_set():
            print('\033[31m红灯亮\033[0m，car%s等着' % n)

            # 阻塞,等待设置等待时间,等待0.1s之后没有等到绿灯就闯红灯走了
            e.wait(0.1)

            if not e.is_set():
                print('\033[33m红灯,警车先走\033[0m,car %s' % n)
            else:
                print('\033[33;46m绿灯,警车走\033[0m,car %s' % n)
        break


def traffic_lights(e, inverval):
    while True:
        time.sleep(inverval)
        if e.is_set():
            print('######', e.is_set())
            # 将is_set()的值设置为False
            e.clear()
        else:
            # 将is_set()的值设置为True
            e.set()
            print('***********', e.is_set())


if __name__ == '__main__':
    e = Event()
    for i in range(10):
        # 创建是个进程控制10辆车
        p = Process(target=car, args=(e, i,))
        p.start()

    for i in range(5):
        # 创建5个进程控制5辆警车
        p = Process(target=police_car, args=(e, i,))
        p.start()

    # 创建一个进程控制红绿灯
    t = Process(target=traffic_lights, args=(e, 10))
    t.start()
```

## 进程间通信

### 队列

#### 概念

- 创建共享的进程队列，Queue是多进程安全的队列，可以使用Queue实现多进程之间的数据传递

  ```
  Queue([maxsize]) 
  创建共享的进程队列。
  参数 ：maxsize是队列中允许的最大项数。如果省略此参数，则无大小限制。
  底层队列使用管道和锁定实现
  ```

- 方法介绍

  ```
  Queue([maxsize]) 
  创建共享的进程队列。maxsize是队列中允许的最大项数。如果省略此参数，则无大小限制。底层队列使用管道和锁定实现。另外，还需要运行支持线程以便队列中的数据传输到底层管道中。 
  Queue的实例q具有以下方法：
  
  q.get( [ block [ ,timeout ] ] ) 
  返回q中的一个项目。如果q为空，此方法将阻塞，直到队列中有项目可用为止。block用于控制阻塞行为，默认为True. 如果设置为False，将引发Queue.Empty异常（定义在Queue模块中）。timeout是可选超时时间，用在阻塞模式中。如果在制定的时间间隔内没有项目变为可用，将引发Queue.Empty异常。
  
  q.get_nowait( ) 
  同q.get(False)方法。
  
  q.put(item [, block [,timeout ] ] ) 
  将item放入队列。如果队列已满，此方法将阻塞至有空间可用为止。block控制阻塞行为，默认为True。如果设置为False，将引发Queue.Empty异常（定义在Queue库模块中）。timeout指定在阻塞模式中等待可用空间的时间长短。超时后将引发Queue.Full异常。
  
  q.qsize() 
  返回队列中目前项目的正确数量。此函数的结果并不可靠，因为在返回结果和在稍后程序中使用结果之间，队列中可能添加或删除了项目。在某些系统上，此方法可能引发NotImplementedError异常。
  
  q.empty() 
  如果调用此方法时 q为空，返回True。如果其他进程或线程正在往队列中添加项目，结果是不可靠的。也就是说，在返回和使用结果之间，队列中可能已经加入新的项目。
  
  q.full() 
  如果q已满，返回为True. 由于线程的存在，结果也可能是不可靠的（参考q.empty（）方法)
  
  q.close() 
  关闭队列，防止队列中加入更多数据。调用此方法时，后台线程将继续写入那些已入队列但尚未写入的数据，但将在此方法完成时马上关闭。如果q被垃圾收集，将自动调用此方法。关闭队列不会在队列使用者中生成任何类型的数据结束信号或异常。例如，如果某个使用者正被阻塞在get（）操作上，关闭生产者中的队列不会导致get（）方法返回错误。
  
  q.cancel_join_thread() 
  不会再进程退出时自动连接后台线程。这可以防止join_thread()方法阻塞。
  
  q.join_thread() 
  连接队列的后台线程。此方法用于在调用q.close()方法后，等待所有队列项被消耗。默认情况下，此方法由不是q的原始创建者的所有进程调用。调用q.cancel_join_thread()方法可以禁止这种行为
  ```

#### 队列的用法

```python
"""
multiprocessing模块支持进程间通信的两种主要形式:管道和队列
都是基于消息传递实现的,但是队列接口
"""

from multiprocessing import Queue

q = Queue(3)

# put ,get ,put_nowait,get_nowait,full,empty
q.put(3)
q.put(3)
q.put(3)
# 如果队列已经满了，程序就会停在这里，等待数据被别人取走，再将数据放入队列。
# 如果队列中的数据一直不被取走，程序就会永远停在这里
# q.put(3)

try:
    # 可以使用put_nowait，如果队列满了不会阻塞，但是会因为队列满了而报错
    # 因此我们可以用一个try语句来处理这个错误。这样程序不会一直阻塞下去，但是会丢掉这个消息
    q.put_nowait(3)
except Exception as ex:
    print('队列已经满了')

# 因此，我们再放入数据之前，可以先看一下队列的状态，如果已经满了，就不继续put了。
# 队列的数据是否存满
print(q.full())  # True

print(q.get())
print(q.get())
print(q.get())
# 同put方法一样，如果队列已经空了，那么继续取就会出现阻塞
# print(q.get())

try:
    # 可以使用get_nowait，如果队列满了不会阻塞，但是会因为没取到值而报错
    # 因此我们可以用一个try语句来处理这个错误。这样程序不会一直阻塞下去
    q.get_nowait(3)
except Exception as ex:
    print('队列已经空了')

# 队列是否为空
print(q.empty())  # True
```

#### 利用队列实现进程间的通信

```python
from multiprocessing import Process
from multiprocessing import Queue
import time
import random


def put_data(queue):
    while 1:
        while queue.full():
            time.sleep(1)
        data = random.randint(1, 100)
        queue.put(data)
        print(f'存放数据 {data}')


def get_data(queue, index):
    while 1:
        data = queue.get()
        print(f'进程{index} 获取到数据 {data}')


if __name__ == '__main__':
    q = Queue(3)
    process_list = []
    for index in range(10):
        p = Process(target=get_data, args=(q, f'{index}'))
        process_list.append(p)

    for item in process_list:
        item.start()

    p = Process(target=put_data, args=(q,))
    p.start()
```

## 生产者消费者模型

在并发编程中使用生产者和消费者模式能够解决绝大多数并发问题。该模式通过平衡生产线程和消费线程的工作能力来提高程序的整体处理数据的速度

- ##### **为什么要使用生产者和消费者模式**

  在线程世界里，生产者就是生产数据的线程，消费者就是消费数据的线程。在多线程开发当中，如果生产者处理速度很快，而消费者处理速度很慢，那么生产者就必须等待消费者处理完，才能继续生产数据。同样的道理，如果消费者的处理能力大于生产者，那么消费者就必须等待生产者。为了解决这个问题于是引入了生产者和消费者模式

- ##### **什么是生产者消费者模式**

  生产者消费者模式是通过一个容器来解决生产者和消费者的强耦合问题。生产者和消费者彼此之间不直接通讯，而通过阻塞队列来进行通讯，所以生产者生产完数据之后不用等待消费者处理，直接扔给阻塞队列，消费者不找生产者要数据，而是直接从阻塞队列里取，阻塞队列就相当于一个缓冲区，平衡了生产者和消费者的处理能力

### 基于队列实现生产者消费者模型

#### 第一版

```python
from multiprocessing import Process
from multiprocessing import Queue
import random
import time


def consumer(queue, index):
    while 1:
        data = queue.get()
        print(f'消费者{index} 消费 {data}')


def producer(queue):
    for index in range(10):
        time.sleep(random.randint(0, 2))
        queue.put(index)
        print(f'生产者 生产 {index}')


if __name__ == '__main__':
    queue = Queue()
    pro = Process(target=producer, args=(queue,))

    consumer_list = []
    for index in range(5):
        p = Process(target=consumer, args=(queue, index))
        consumer_list.append(p)

    for item in consumer_list:
        item.start()

    pro.start()
```

- 问题

  1. 主进程永远不会结束

     生产者p在生产完后就结束了，但是消费者c在取空了q之后，则一直处于死循环中且卡在q.get()这一步

- 解决方法

  让生产者在生产完毕后，往队列中再发一个结束信号，这样消费者在接收到结束信号后就可以break出死循环

#### 第二版

```python
from multiprocessing import Process
from multiprocessing import Queue
import random
import time


def consumer(queue, index):
    while 1:
        data = queue.get()

        if not data:
            print(f'消费者进程{index} 结束')
            break

        print(f'消费者{index} 消费 {data}')


def producer(queue, consumer_size):
    for index in range(1, 10):
        time.sleep(random.randint(0, 2))
        queue.put(index)
        print(f'生产者 生产 {index}')

    for index in range(consumer_size):
        queue.put(None)


if __name__ == '__main__':
    # 消费者的个数
    consumer_size = 5
    # 创建队列
    queue = Queue()

    pro = Process(target=producer, args=(queue, consumer_size))

    consumer_list = []
    for index in range(consumer_size):
        p = Process(target=consumer, args=(queue, index))
        consumer_list.append(p)

    for item in consumer_list:
        item.start()

    pro.start()
```

**上述虽然解决了主进程无法关闭的问题, 但是缺点在于消费者生产数据完毕后, 对于 None 的传入, 有多少个消费者就必须传入指定个 None值**

#### 第三版

- JoinableQueue

  创建可连接的共享进程队列。这就像是一个Queue对象，但队列允许项目的使用者通知生产者项目已经被成功处理。通知进程是使用共享的信号和条件变量来实现的

  ```
  JoinableQueue的实例p除了与Queue对象相同的方法之外，还具有以下方法：
  
  q.task_done() 
  使用者使用此方法发出信号，表示q.get()返回的项目已经被处理。如果调用此方法的次数大于从队列中删除的项目数量，将引发ValueError异常。
  
  q.join() 
  生产者将使用此方法进行阻塞，直到队列中所有项目均被处理。阻塞将持续到为队列中的每个项目均调用q.task_done()方法为止。 
  下面的例子说明如何建立永远运行的进程，使用和处理队列上的项目。生产者将项目放入队列，并等待它们被处理。
  ```

- 多生产者多消费者模型

  ```python
  from multiprocessing import Process
  from multiprocessing import JoinableQueue
  import random
  import time
  
  
  def consumer(queue, consumer_name):
      while 1:
          data = queue.get()
          print(f'消费者{consumer_name} 消费 {data}')
          # 向q.join()发送一次信号,证明一个数据已经被取走了
          queue.task_done()
  
  
  def producer(queue, producer_name):
      for index in range(1, 5):
          time.sleep(random.randint(0, 2))
          queue.put(index)
          print(f'生产者{producer_name} 生产 {index}')
  
      # 生产完毕，使用此方法进行阻塞，直到队列中所有项目均被处理
      queue.join()
  
  
  if __name__ == '__main__':
      # 生产者个数
      producer_size = 3
      # 消费者的个数
      consumer_size = 5
      # 创建队列
      queue = JoinableQueue()
  
      producer_list = []
      for index in range(producer_size):
          pro = Process(target=producer, args=(queue, index))
          producer_list.append(pro)
  
      consumer_list = []
      for index in range(consumer_size):
          # 将消费者进程设置为守护进程
          p = Process(target=consumer, args=(queue, index))
          p.daemon = True
          consumer_list.append(p)
  
      for item in consumer_list:
          item.start()
  
      for item in producer_list:
          item.start()
  
      for item in producer_list:
          # 主进程等待生产者进程执行完毕
          item.join()
  ```

## 管道(了解)

- 对于多生产者多消费者, 没有做到数据的安全. 如果需要做到数据的安全, 需要另外进行加锁处理
- multiprocessing 模块的 Queue = 管道 + 锁

### 参数和方法介绍

- 创建管道的类

  Pipe([duplex]):在进程之间创建一条管道，并返回元组（conn1,conn2）,其中conn1，conn2表示管道两端的连接对象，强调一点：必须在产生Process对象之前产生管道

- 参数介绍

  dumplex:默认管道是全双工的，如果将duplex射成False，conn1只能用于接收，conn2只能用于发送

- 主要方法

  - conn1.recv():接收conn2.send(obj)发送的对象。如果没有消息可接收，recv方法会一直阻塞。如果连接的另外一端已经关闭，那么recv方法会抛出EOFError
  - conn1.send(obj):通过连接发送对象。obj是与序列化兼容的任意对象

- 其他方法

  - conn1.close():关闭连接。如果conn1被垃圾回收，将自动调用此方法
  - conn1.fileno():返回连接使用的整数文件描述符
  - conn1.poll([timeout]):如果连接上的数据可用，返回True。timeout指定等待的最长时限。如果省略此参数，方法将立即返回结果。如果将timeout射成None，操作将无限期地等待数据到达
  - conn1.recv_bytes([maxlength]):接收c.send_bytes()方法发送的一条完整的字节消息。maxlength指定要接收的最大字节数。如果进入的消息，超过了这个最大值，将引发IOError异常，并且在连接上无法进行进一步读取。如果连接的另外一端已经关闭，再也不存在任何数据，将引发EOFError异常
  - conn.send_bytes(buffer [, offset [, size]])：通过连接发送字节数据缓冲区，buffer是支持缓冲区接口的任意对象，offset是缓冲区中的字节偏移量，而size是要发送字节数。结果数据以单条消息的形式发出，然后调用c.recv_bytes()函数进行接收 
  - conn1.recv_bytes_into(buffer [, offset]):接收一条完整的字节消息，并把它保存在buffer对象中，该对象支持可写入的缓冲区接口（即bytearray对象或类似的对象）。offset指定缓冲区中放置消息处的字节位移。返回值是收到的字节数。如果消息长度大于可用的缓冲区空间，将引发BufferTooShort异常

### 初步使用

```python
from multiprocessing import Pipe
from multiprocessing import Process


def func(pipe_right):
    data = pipe_right.recv()
    print(f'子进程收到数据 {data}')

    pipe_right.send('123')
    pipe_right.close()


if __name__ == '__main__':
    pipe_left, pipe_right = Pipe()

    process = Process(target=func, args=(pipe_right,))
    process.start()

    pipe_right.close()

    pipe_left.send('abc')
    data = pipe_left.recv()
    print(f'主进程接收数据 {data}')
```

### 注意点

特别注意管道端点的正确管理问题。**如果是生产者或消费者中都没有使用管道的某个端点，就应将它关闭**。这也说明了为何在**生产者中关闭了管道的输出端，在消费者中关闭管道的输入端**。如果忘记执行这些步骤，程序可能在消费者中的recv（）操作上挂起。管道是由操作系统进行引用计数的，必须在所有进程中关闭管道后才能生成EOFError异常。因此，在生产者中关闭管道不会有任何效果，除非消费者也关闭了相同的管道端点

### EOFError

```python
from multiprocessing import Process, Pipe


def f(parent_conn, child_conn):
    parent_conn.close()  # 不写close将不会引发EOFError
    while True:
        try:
            print(child_conn.recv())
        except EOFError:
            child_conn.close()


if __name__ == '__main__':
    parent_conn, child_conn = Pipe()
    p = Process(target=f, args=(parent_conn, child_conn,))
    p.start()
    child_conn.close()
    parent_conn.send('hello')
    parent_conn.close()
    p.join()
```

### 利用管道实现进程间通信

```python
from multiprocessing import Process, Pipe


def consumer(p, name):
    produce, consume = p
    produce.close()
    while True:
        try:
            baozi = consume.recv()
            print('%s 收到包子:%s' % (name, baozi))
        except EOFError:
            break


def producer(seq, p):
    produce, consume = p
    consume.close()
    for i in seq:
        produce.send(i)


if __name__ == '__main__':
    produce, consume = Pipe()

    c1 = Process(target=consumer, args=((produce, consume), 'c1'))
    c1.start()

    seq = (i for i in range(10))
    producer(seq, (produce, consume))

    produce.close()
    consume.close()

    c1.join()
    print('主进程')
```

### 多消费者竞争带来的数据不安全问题

```python
from multiprocessing import Process, Pipe, Lock


def consumer(p, name, lock):
    produce, consume = p
    produce.close()
    while True:
        lock.acquire()
        baozi = consume.recv()
        lock.release()
        if baozi:
            print('%s 收到包子:%s' % (name, baozi))
        else:
            consume.close()
            break


def producer(p, n):
    produce, consume = p
    consume.close()
    for i in range(n):
        produce.send(i)
    produce.send(None)
    produce.send(None)
    produce.close()


if __name__ == '__main__':
    produce, consume = Pipe()
    lock = Lock()
    c1 = Process(target=consumer, args=((produce, consume), 'c1', lock))
    c2 = Process(target=consumer, args=((produce, consume), 'c2', lock))
    p1 = Process(target=producer, args=((produce, consume), 10))
    c1.start()
    c2.start()
    p1.start()

    produce.close()
    consume.close()

    c1.join()
    c2.join()
    p1.join()
    print('主进程')
```

## Manager(了解)

即便是使用线程,推荐做法也是将程序设计为大量独立的线程集合,通过消息队列交换数据.这样极大地减少了对使用锁定和其他同步手段的需求，还可以扩展到分布式系统中。
**但进程间应该尽量避免通信，即便需要通信，也应该选择进程安全的工具来避免加锁带来的问题**
以后我们会尝试使用数据库来解决现在进程之间的数据共享问题

```
进程间数据是独立的，可以借助于队列或管道实现通信，二者都是基于消息传递的
虽然进程间数据独立，但可以通过Manager实现数据共享，事实上Manager的功能远不止于此
```

- 示例

  ```python
  # Pipe 管道 双向通信 数据不安全
  # Queue 管道+锁  双向通信 数据安全
  
  # JoinableQueue ：数据安全
  # put 和 get的一个计数机制，每次get数据之后发送task_done，put端接收到计数-1，直到计数为0就能感知到
  
  # Manager是一个类,就提供了可以进行数据共享的一个机制,提供了很多数据类型.如dict,list等
  
  from multiprocessing import Manager, Process
  import time
  import random
  
  
  def get_data(dic):
      while True:
          print(dic)
          time.sleep(1)
  
  
  def set_data(dic):
      for item in range(10):
          dic['count'] = item
          time.sleep(random.randint(0, 2))
  
  
  if __name__ == '__main__':
      m = Manager()
      d = m.dict()
  
      producer = Process(target=set_data, args=(d,))
  
      consumer = Process(target=get_data, args=(d,))
  
      producer.start()
      consumer.start()
  
      producer.join()
      consumer.join()
  ```

  ```python
  from multiprocessing import Manager, Process, Lock
  
  
  def work(d, lock):
      with lock:  # 不加锁而操作共享的数据,肯定会出现数据错乱
          d['count'] -= 1
  
  
  if __name__ == '__main__':
      lock = Lock()
      with Manager() as m:
          dic = m.dict({'count': 100})
          p_l = []
          for i in range(100):
              p = Process(target=work, args=(dic, lock))
              p_l.append(p)
              p.start()
          for p in p_l:
              p.join()
          print(dic)
  ```

## 进程池

在程序实际处理问题过程中，忙时会有成千上万的任务需要被执行，闲时可能只有零星任务。那么在成千上万个任务需要被执行的时候，我们就需要去创建成千上万个进程么？首先，创建进程需要消耗时间，销毁进程也需要消耗时间。第二即便开启了成千上万的进程，操作系统也不能让他们同时执行，这样反而会影响程序的效率。因此我们**不能无限制的根据任务开启或者结束进程**

进程池的概念，定义一个池子，在里面放上固定数量的进程，有需求来了，就拿一个池中的进程来处理任务，等到处理完毕，进程并不关闭，而是将进程再放回进程池中继续等待任务。如果有很多任务需要执行，池中的进程数量不够，任务就要等待之前的进程执行任务完毕归来，拿到空闲进程才能继续执行。也就是说，池中进程的数量是固定的，那么同一时间最多有固定数量的进程在运行。这样不会增加操作系统的调度难度，还节省了开闭进程的时间，也一定程度上能够实现并发效果

### 概念

```
Pool([numprocess  [,initializer [, initargs]]]):创建进程池

numprocess:要创建的进程数，如果省略，将默认使用cpu_count()的值
initializer：是每个工作进程启动时要执行的可调用对象，默认为None
initargs：是要传给initializer的参数组
```

- 主要方法

```
p.apply(func [, args [, kwargs]]):在一个池工作进程中执行func(*args,**kwargs),然后返回结果。
'''
需要强调的是：此操作并不会在所有池工作进程中并执行func函数。如果要通过不同参数并发地执行func函数，必须从不同线程调用p.apply()函数或者使用p.apply_async()
'''
 
p.apply_async(func [, args [, kwargs]]):在一个池工作进程中执行func(*args,**kwargs),然后返回结果。
'''
此方法的结果是AsyncResult类的实例，callback是可调用对象，接收输入参数。当func的结果变为可用时，将理解传递给callback。callback禁止执行任何阻塞操作，否则将接收其他异步操作中的结果
'''
    
p.close():关闭进程池，防止进一步操作。如果所有操作持续挂起，它们将在工作进程终止前完成

P.jion():等待所有工作进程退出。此方法只能在 close（）或 teminate()之后调用
```

- 其他方法

```
方法apply_async()和map_async（）的返回值是AsyncResul的实例obj。实例具有以下方法

obj.get():返回结果，如果有必要则等待结果到达。timeout是可选的。如果在指定时间内还没有到达，将引发一场。如果远程操作中引发了异常，它将在调用此方法时再次被引发。

obj.ready():如果调用完成，返回True

obj.successful():如果调用完成且没有引发异常，返回True，如果在结果就绪之前调用此方法，引发异常
obj.wait([timeout]):等待结果变为可用。

obj.terminate()：立即终止所有工作进程，同时不执行任何清理或结束任何挂起工作。如果p被垃圾回收，将自动调用此函数
```

- 创建进程池的其他方式

```python
from concurrent.futures import ProcessPoolExecutor
import multiprocessing
import time


def get_current_process():
    return multiprocessing.current_process().name


def task(arg):
    print(f"{get_current_process()} 执行任务, arg : {arg}")
    time.sleep(2)
    return arg ** 2


# 创建进程池
pool = ProcessPoolExecutor(2)

feature_list = list()
for item in range(10):
    # 通过进程池执行任务
    feature_list.append(pool.submit(task, item))

for item in feature_list:
    # 获取任务的结果
    result_data = item.result()
    print(f"{get_current_process()} 获取结果, result_data : {result_data}")
```

### 进程池和多进程效率的对比

```python
from multiprocessing import Pool
from multiprocessing import Process
import time
import os


def add(i):
    return i + 1


def test_pool():
    start_time = time.time()

    process_size = os.cpu_count() + 1

    p = Pool(process_size)
    # map 方法
    # 第一个参数: 传递方法的名称
    # 第二个参数: 传入可迭代对象, 内部会依次取每个数据, 然后调用方法
    p.map(add, range(3000))

    # 关闭进程池, 防止进一步操作
    # 如果所有操作持续挂起, 它们将在工作进程终止前完成
    p.close()

    print(f'进程池消耗 {time.time() - start_time}')


def test_process():
    start_time = time.time()

    process_list = []
    for item in range(3000):
        p = Process(target=add, args=(item,))
        process_list.append(p)

    [item.start() for item in process_list]
    [item.join() for item in process_list]

    print(f'不使用进程池消耗 {time.time() - start_time}')


if __name__ == '__main__':
    test_pool()
    test_process()

# 进程池消耗 0.015331029891967773
# 不使用进程池消耗 3.336879014968872
```

### 进程池的同步调用

```python
def get_pow_2(i):
    res = i ** 2
    current_time = time.strftime('%X')
    print(f'{current_time} , res = {res}')
    time.sleep(1)
    return res


if __name__ == '__main__':
    p = Pool(os.cpu_count() + 1)

    for item in range(10):
        # 同步调用，直到本次任务执行完毕拿到res，等待任务work执行的过程中可能有阻塞也可能没有阻塞
        # 但不管该任务是否存在阻塞，同步调用都会在原地等着
        value = p.apply(get_pow_2, (item,))

# 16:37:14 , res = 0
# 16:37:15 , res = 1
# 16:37:16 , res = 4
# 16:37:17 , res = 9
# 16:37:18 , res = 16
# 16:37:19 , res = 25
# 16:37:20 , res = 36
# 16:37:21 , res = 49
# 16:37:22 , res = 64
# 16:37:23 , res = 81
```

### 进程池的异步调用

```python
def get_pow_2(i):
    res = i ** 2
    current_time = time.strftime('%X')
    print(f'{current_time} , res = {res}')
    time.sleep(1)
    return res


if __name__ == '__main__':
    p = Pool(os.cpu_count() + 1)

    result_list = []

    for item in range(10):
        # 异步运行,根据进程池中有的进程数,每次最多 os.cpu_count() + 1 个子进程在异步执行
        # 返回结果之后,将结果放入列表,归还进程,之后再执行新的任务
        # 需要注意的是,进程池中的三个进程不会同时开启或者同时结束
        # 而是执行完一个就释放一个进程,这个进程就去接收新的任务
        value = p.apply_async(get_pow_2, (item,))
        result_list.append(value)

    # 异步apply_async用法: 如果使用异步提交的任务,主进程需要使用jion,等待进程池内任务都处理完,然后可以用get收集结果
    # 否则,主进程结束,进程池可能还没来得及执行,也就跟着一起结束了
    p.close()
    p.join()

    for res in result_list:
        # 使用get来获取apply_aync的结果, 如果是apply, 则没有get方法, 因为apply是同步执行, 立刻获取结果, 也根本无需get
        print(res.get())
```

## 进程池版爬虫案例

```python
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor
import multiprocessing
import requests
import re


def request_url_data(url):
    return requests.get(url).text


def get_page_url(page_index):
    index = (page_index - 1) * 25
    return f"https://movie.douban.com/top250?start={index}"


def request_page_data(url):
    print(f"当前进程 : {multiprocessing.current_process().name} 执行 {url}")
    response_data = request_url_data(url)
    data_res_iter = re_c.finditer(response_data)

    response_list = list()
    for item_page_data in data_res_iter:
        item_data = {
            "title": item_page_data.group("title"),
            "rating": item_page_data.group("rating"),
            "count": item_page_data.group("count"),
            "desc": item_page_data.group("desc"),
        }
        response_list.append(item_data)
    return response_list


def handle_response_data(response_data_feature):
    for item_response in response_data_feature.result():
        print(item_response)


if __name__ == '__main__':
    pattern = r'<div class="item">.+?<span class="title">(?P<title>.+?)</span>.+?' \
              r'<span class="rating_num".+?>(?P<rating>.+?)</span>.+?' \
              r'<span>(?P<count>.+?评价)</span>.+?' \
              r'<span class="inq">(?P<desc>.+?)</span>'
    re_c = re.compile(pattern, re.S)

    # 创建进程池
    pool = ProcessPoolExecutor(4)

    # 创建线程池
    # pool = ThreadPoolExecutor(4)

    for item_page_index in range(1, 10):
        page_url = get_page_url(item_page_index)
        pool.submit(request_page_data, page_url).add_done_callback(handle_response_data)
```


